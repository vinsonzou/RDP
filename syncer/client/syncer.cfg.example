# RDP syncer sample config file

# MySQL Cluster virtual Ip address, managed via MHA suite,
# For RDP, we always connect with unique virtual Ip
mysql.vip=10.199.134.2xx

# MySQL Cluster port number
mysql.port=3398

# User name connect to MySQL
mysql.user=test

# Passcode connect to MySQL
mysql.password=123

# Mysql version which is supported
mysql.version=5.7.17-log

# Max net io rate when fetch binlog from master, in MB/s
mysql.max.rate=10

# Syncer group id, used to identify a RDP GROUP
group.id=1000x


# RDP syncer is a MySQL slave, thus a UUID required
syncer.uuid=b3927e40-dc69-22e6-83a8-90b11c3c9447

# The period of heartbeat from master, in seconds
syncer.heartbeat.period=600

# Host is used to identify RDP process in a RDP GROUP
syncer.host=8.8.8.8

# RDP syncer is a MySQL slave, thus a server id required
server.id=12345

# Log detail binlog events info 
debug.info=0

# Switch to enable/disable MySQL semi-sync replication
semi.sync=0

# Interval in second try to reconnect to MySQL Master
# in case of connection lost
retry.interval=5

# Switch to enable/disable fencing token verification,
# requires MySQL kernel (mysqld) upgraded
fencing.token.enable=0

# Log file location and log level, zero is the verbosest
log.dir=/apps/svr/rdp_syncer/data/1000x/logs
log.level=0

# buffer pool size in MB for slave thread
memory.pool.size=4096

# If write binlog events to local binlog files
dump.local.file=0

# If dump.local.file enabled, config the fsync method
sync.method=0

# If dump.local.file enabled, config file location
binlog.dir=./binlog

# Directory where scripts locate
script.dir=../scripts

# Info about the schema meta db
schema.meta.db.host=10.199.134.2xx
schema.meta.db.port=3398
schema.meta.db.user=root
schema.meta.db.password=123
schema.meta.db.database=db_rdp_schema

# Info about the schema executer db,
# it can be same with the meta db
schema.executer.db.host=10.199.134.2xx
schema.executer.db.port=3398
schema.executer.db.user=root
schema.executer.db.password=123

# MTS setting
parsing.threads.num=5

#add original event binlog to parsing result 
parsing.with_orig_buf=0

# Buffer to buff the trx msg
trx.buffer.size=50000

# max transaction size that rdp can parse
trx.max_trans_size=4096000000


# Zookeeper log level
zk.loglevel=DEBUG
# Zookeeper hosts list
zk.hosts=10.199.134.210:2181,10.199.134.211:2181,10.198.189.253:2181
# Zookeeper recv timeout ms
zk.recvtimeout=10000
# Zookeeper log file prefix name
zk.log.name=zookeeper
zk.log.size=134217728 #128M
zk.log.days=7

# Zookeeper path store all election nodes
node.list.path=/rdp_syncer/1000x/nodes
# Zookeeper path store curr leader info
leader.path=/rdp_syncer/1000x/leader

filter.path=/rdp_syncer/1000x/filter

# Zookeeper path store checkpoint info
checkpoint.path=/rdp_syncer/1000x/checkpoint
# checkpoint interval time ms
checkpoint.interval_ms=5000
# checkpoint thread release memory interval time ms
checkpoint.release_mem_ms=100

kafka.b_version_fback=0.8.1
# Kafka broker lists
kafka.brokerlist=10.199.134.210:9092,10.199.134.211:9092,10.198.189.253:9092
# Kafka topic name
kafka.topic=test1
# Kafka topic partition
kafka.partition=0
# Kafka consumer read max message size
kafka.consumer.fetch_max_msg_bytes=1000000
# Kafkf producer write message required broker response number
kafka.producer.acks=-1
# Rdkafka producer batch message number
kafka.producer.batch_num_msg=500
# Rdkafka producer batch message queue length
kafka.producer.qbuf_max_ms=1000
# Rdkafka store max message size
kafka.producer.msg_max_bytes=1000000
# Max split message size
kafka.producer.split_msg_bytes=900000
# enable split message
kafka.producer.enable_split_msg=0
# consumer message rate limit Mb
kafka.comsumer.rate.limit.mb=100
# Google Protobuf checksum enable flag
checksum.enable=0
# compress transaction flag
compress.enable=0
# decompress buffer max size 10M
decompress.max.buf.size=10485760
# Encode message map size
msg.map.size=3000
# RdKafka Producer Queue Buffer Max Message Number
kafka.producer.q_buf_max_msgs=5000
# Rdkafka Producer Queue Buffer Max KBytes 1..2097151
kafka.producer.q_buf_max_kb=1048576
# Producer Pop Once Max Encoded Map Number
kafka.producer.pop_map_msgs=20
# kafka producer message.send.max.retries
kafka.producer.send_max_retry=0
# write kafka no response timeout ms
kafka.no_rsp_timeout_ms=100000
# Kafka metadata refresh interval, in ms
kafka.metadata_refresh_interval_ms=30000
# kafka producer can merge empty transaction
kafka.producer.enable_trans_merge=0
# kafka producer log.connection.close
# default is false, if librdkafka some sock close not call event_cb, program not exit
# if all broker is down, will call event_cb, program exit
# kafka.producer.log_cn_close=false
# kafka producer debug model
# default is "", not open librdkafka producer debug
# debug level:generic, broker, topic, metadata, queue, msg, protocol, cgrp, security, fetch, feature, all
# kafka.debug=all
# Check All Thread Alive And Metric Resource Time ms
ckthdmtrs.time_ms=3000
# statistics Interval
stat.interval_ms=6000


# The namespace of these metrics
metrics.namespace=app
# Directory to place the metric files
metrics.file.dir=/apps/svr/rdp_syncer/data/1000x/metrics
metrics.file.name=rdp_syncer
# Max file size, in bytes
metrics.file.size=90000000
# Max days to reserve the files
metrics.file.days=7

# Read after write feature
slaveview.read_after_write=0
slaveview.slave_server_ids=123456,123457
# Interval to refresh slave list
slaveview.refresh_view_interval=60000
# Timeout to wait a gtid
slaveview.wait_timeout=5000
# Interval to check slave healthy and get gtid set when idle
slaveview.check_slave_interval_idle=10000
# Interval to check slave healthy and get gtid set when busy
slaveview.check_slave_interval_busy=500
